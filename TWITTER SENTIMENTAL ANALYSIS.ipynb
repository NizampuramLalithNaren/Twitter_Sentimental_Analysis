{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification\n",
    "In supervised classification, the classifier is trained with labeled training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK’s twitter_samples corpus is used as labeled training data.\n",
    "     The twitter_samples corpus contains 3 files.\n",
    "\n",
    "    1) negative_tweets.json: contains 5k negative tweets\n",
    "    2) positive_tweets.json: contains 5k positive tweets\n",
    "    3) tweets.20150430-223406.json: contains 20k positive and negative tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "print (twitter_samples.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "print (len(pos_tweets)) \n",
    " \n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print (len(neg_tweets))\n",
    " \n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "print (len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "@97sides CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
     ]
    }
   ],
   "source": [
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Tweet\n",
    "\n",
    "In the tweet cleaning process, we will do the following:\n",
    "\n",
    "    – Remove stock market tickers like $GE\n",
    "    – Remove retweet text “RT”\n",
    "    – Remove hyperlinks\n",
    "    – Remove hashtags (only the hashtag # and not the word)\n",
    "    – Remove stop words like a, and, the, is, are, etc.\n",
    "    – Remove emoticons like :), :D, :(, :-), etc.\n",
    "    – Remove punctuation like full-stop, comma, exclamation sign, etc.\n",
    "    – Convert words to Stem/Base words using Porter Stemming Algorithm. E.g. words like ‘working’, ‘works’, and ‘worked’ will be converted to their base/stem word “work”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, \n",
    "                                 strip_handles=True,\n",
    "                                 reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    tweet = re.sub(r'\\$\\w*','',tweet)\n",
    "    \n",
    "    tweet = re.sub(r'^RT[\\s]+','',tweet)\n",
    "    \n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',tweet)\n",
    "    \n",
    "    tweet = re.sub(r'#','',tweet)\n",
    "    \n",
    "    tweet_tokens = tweet_tokenizer.tokenize(tweet)\n",
    "    \n",
    "    \n",
    "    tweet_clean=[]\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and word not in emoticons and \n",
    "            word not in string.punctuation):\n",
    "                stem_word = stemmer.stem(word)\n",
    "                tweet_clean.append(stem_word)\n",
    "    return tweet_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "We define a simple bag_of_words function that extracts features from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive tweets feature set\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos'))    \n",
    " \n",
    "#negative tweets feature set\n",
    "neg_tweets_set = []\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle \n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    " \n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734\n",
      "Most Informative Features\n",
      "                     via = True              pos : neg    =     37.7 : 1.0\n",
      "                     sad = True              neg : pos    =     29.0 : 1.0\n",
      "                     bam = True              pos : neg    =     25.0 : 1.0\n",
      "                     x15 = True              neg : pos    =     18.3 : 1.0\n",
      "                 appreci = True              pos : neg    =     16.3 : 1.0\n",
      "                   arriv = True              pos : neg    =     15.9 : 1.0\n",
      "                     ugh = True              neg : pos    =     15.0 : 1.0\n",
      "                      ff = True              pos : neg    =     14.6 : 1.0\n",
      "                      aw = True              neg : pos    =     14.2 : 1.0\n",
      "                    glad = True              pos : neg    =     13.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy) # Output: 0.765\n",
    " \n",
    "print (classifier.show_most_informative_features(10))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
